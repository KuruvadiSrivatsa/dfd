{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCOY129FWgXw",
        "outputId": "724d36de-7444-4114-91b9-af38561043df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deepfake-detection-challenge.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  deepfake-detection-challenge.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace test_videos/aassnaulhq.mp4? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace test_videos/aayfryxljh.mp4? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# !kaggle competitions download -c deepfake-detection-challenge\n",
        "\n",
        "# !unzip deepfake-detection-challenge.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY8qNssHXIr2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.signal import butter, filtfilt\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the pre-trained face detector\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def extract_frames_and_detect_faces(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "        for (x, y, w, h) in faces:\n",
        "            roi_color = frame[y:y+h, x:x+w]\n",
        "            frames.append(roi_color)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def extract_green_channel_signal(frames):\n",
        "    signals = []\n",
        "    for frame in frames:\n",
        "        height, width, _ = frame.shape\n",
        "        forehead_region = frame[:height//3, :]\n",
        "        green_channel = forehead_region[:, :, 1]\n",
        "        mean_green_value = np.mean(green_channel)\n",
        "        signals.append(mean_green_value)\n",
        "    return signals\n",
        "\n",
        "def bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = filtfilt(b, a, signal)\n",
        "    return y\n",
        "\n",
        "def create_spatiotemporal_map(signal, width, height):\n",
        "    spatiotemporal_map = np.zeros((height, width))\n",
        "    signal_length = len(signal)\n",
        "    for i in range(height):\n",
        "        for j in range(width):\n",
        "            if i < signal_length:\n",
        "                spatiotemporal_map[i, j] = signal[i]\n",
        "    return spatiotemporal_map\n",
        "\n",
        "def process_video(video_path, map_width=50, lowcut=0.8, highcut=2.5, fs=30):\n",
        "    frames = extract_frames_and_detect_faces(video_path)\n",
        "    green_channel_signal = extract_green_channel_signal(frames)\n",
        "    filtered_signal = bandpass_filter(green_channel_signal, lowcut, highcut, fs)\n",
        "    map_height = len(filtered_signal)\n",
        "    spatiotemporal_map = create_spatiotemporal_map(filtered_signal, map_width, map_height)\n",
        "    return spatiotemporal_map\n",
        "\n",
        "def process_videos_in_folder(folder_path, metadata_path):\n",
        "    # Load metadata\n",
        "    with open(metadata_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    spatiotemporal_maps = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.mp4') and filename in metadata:\n",
        "            video_path = os.path.join(folder_path, filename)\n",
        "            spatiotemporal_map = process_video(video_path)\n",
        "            spatiotemporal_maps.append(spatiotemporal_map)\n",
        "            label = 1 if metadata[filename]['label'] == 'REAL' else 0\n",
        "            labels.append(label)\n",
        "    return np.array(spatiotemporal_maps), np.array(labels)\n",
        "\n",
        "# Example folder path and metadata path\n",
        "folder_path = 'train_sample_videos'\n",
        "metadata_path = os.path.join(folder_path, 'metadata.json')\n",
        "spatiotemporal_maps, labels = process_videos_in_folder(folder_path, metadata_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJAgnXFkZ2V9"
      },
      "outputs": [],
      "source": [
        "# Add channel dimension for grayscale images\n",
        "spatiotemporal_maps = np.expand_dims(spatiotemporal_maps, axis=-1)\n",
        "\n",
        "# Shuffle the data\n",
        "indices = np.arange(len(labels))\n",
        "np.random.shuffle(indices)\n",
        "spatiotemporal_maps = spatiotemporal_maps[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(spatiotemporal_maps, labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE5MIylAZ8pi"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "input_shape = (spatiotemporal_maps.shape[1], spatiotemporal_maps.shape[2], 1)\n",
        "model = build_cnn_model(input_shape)\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPRxbwp7aCkN"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Predict on new videos\n",
        "def predict_video(video_path):\n",
        "    spatiotemporal_map = process_video(video_path)\n",
        "    spatiotemporal_map = np.expand_dims(spatiotemporal_map, axis=0)\n",
        "    spatiotemporal_map = np.expand_dims(spatiotemporal_map, axis=-1)\n",
        "    prediction = model.predict(spatiotemporal_map)\n",
        "    return 'Real' if prediction[0][0] > 0.5 else 'Fake'\n",
        "\n",
        "# Example prediction\n",
        "test_video_path = '/content/test_videos/aassnaulhq.mp4'\n",
        "print(predict_video(test_video_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SGGLh-WaPbo"
      },
      "outputs": [],
      "source": [
        "# Define the file path to save the entire model\n",
        "model_file_path = 'model.h5'\n",
        "\n",
        "# Save the entire model\n",
        "model.save(model_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpxXQCDla_jh"
      },
      "outputs": [],
      "source": [
        "# Define the file path to save the model weights\n",
        "weights_file_path = 'model_weights.h5'\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights(weights_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKB3OAaXaeAk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.signal import butter, filtfilt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Load the pre-trained face detector\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def extract_frames_and_detect_faces(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "        for (x, y, w, h) in faces:\n",
        "            roi_color = frame[y:y+h, x:x+w]\n",
        "            frames.append(roi_color)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def extract_green_channel_signal(frames):\n",
        "    signals = []\n",
        "    for frame in frames:\n",
        "        height, width, _ = frame.shape\n",
        "        forehead_region = frame[:height//3, :]\n",
        "        green_channel = forehead_region[:, :, 1]\n",
        "        mean_green_value = np.mean(green_channel)\n",
        "        signals.append(mean_green_value)\n",
        "    return signals\n",
        "\n",
        "def bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    y = filtfilt(b, a, signal)\n",
        "    return y\n",
        "\n",
        "def create_spatiotemporal_map(signal, width, height):\n",
        "    spatiotemporal_map = np.zeros((height, width))\n",
        "    signal_length = len(signal)\n",
        "    for i in range(height):\n",
        "        for j in range(width):\n",
        "            if i < signal_length:\n",
        "                spatiotemporal_map[i, j] = signal[i]\n",
        "    return spatiotemporal_map\n",
        "\n",
        "def process_video(video_path, map_width=50, lowcut=0.8, highcut=2.5, fs=30):\n",
        "    frames = extract_frames_and_detect_faces(video_path)\n",
        "    green_channel_signal = extract_green_channel_signal(frames)\n",
        "    filtered_signal = bandpass_filter(green_channel_signal, lowcut, highcut, fs)\n",
        "    map_height = len(filtered_signal)\n",
        "    spatiotemporal_map = create_spatiotemporal_map(filtered_signal, map_width, map_height)\n",
        "    return spatiotemporal_map\n",
        "\n",
        "# If you saved the entire model\n",
        "model_file_path = 'model.h5'\n",
        "model = load_model(model_file_path)\n",
        "\n",
        "# If you saved just the model weights\n",
        "def build_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "input_shape = (None, 50, 1)  # Adjust the input shape if necessary\n",
        "model = build_cnn_model(input_shape)\n",
        "weights_file_path = 'model_weights.h5'\n",
        "model.load_weights(weights_file_path)\n",
        "\n",
        "# Predict on new videos\n",
        "def predict_video(video_path):\n",
        "    spatiotemporal_map = process_video(video_path)\n",
        "    spatiotemporal_map = np.expand_dims(spatiotemporal_map, axis=0)\n",
        "    spatiotemporal_map = np.expand_dims(spatiotemporal_map, axis=-1)\n",
        "    prediction = model.predict(spatiotemporal_map)\n",
        "    return 'Real' if prediction[0][0] > 0.5 else 'Fake'\n",
        "\n",
        "# Example prediction\n",
        "test_video_path = '/content/test_videos/aassnaulhq.mp4'\n",
        "print(predict_video(test_video_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anikz8dIbM9H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
